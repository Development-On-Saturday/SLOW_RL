{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 2. 마르코프 결정 프로세스 (Markov Decision Process) \n",
    "<br><br>\n",
    "\n",
    "문제를 풀기 위해서는 먼저 문제가 잘 정의되어야 합니다. <br>\n",
    "강화학습에서 문제를 잘 정의하려면 주어진 문제를 MDP (Markov Desicion Process) 의 형태로 만들어야 합니다.\n",
    "<br><br>\n",
    "마르코프 결정 프로세스 (MDP) 를 이해하는 과정 <br>\n",
    "마르코프 프로세스 (MP) &ensp; -> &ensp; 마르코프 리워드 프로세스 &ensp; -> &ensp; 마르코프 결정 프로세스 (MDP)\n",
    "<br><br><br>\n",
    "\n",
    "\n",
    "### Preview <br><br>\n",
    "\n",
    "- __강화학습__ \n",
    "<br><br>\n",
    "    추상적 의미 : 순차적 의사결정 문제를 푸는 방법론 <br>\n",
    "    구체적 의미 : Markov Decision Process 문제를 푸는 방법론 \n",
    "    <br><br>\n",
    "    \"강화 학습을 푼다는 것은 최적의 정책 함수를 찾는 것과 같다.\" <br>\n",
    "    \"그리고 이 최적의 정책 함수는 불확실한 미래에 얻을 수 있는 보상 함수의 기대값을 최대로 하는 행동을 매번 고른다.\" <br>\n",
    "    여기서 눈여겨볼 단어가 두 개 있는데 하나는 ‘__미래__’ 라는 것이고, 두 번째는 ‘__기대값__’ 이라는 것입니다. <br>\n",
    "    이 두 의미만 제대로 깨달아도 강화 학습에 대해서 어느 정도는 이해했다고 볼 수 있습니다. <br>\n",
    "    [출처 - [카카오AI리포트]알파고를 탄생시킨 강화학습의 비밀](https://brunch.co.kr/@kakao-it/73) \n",
    "<br><br>\n",
    "\n",
    "- __마르코프 결정 프로세스 (Markov Decision Process)__\n",
    "<br><br>\n",
    "    연구자들은 이 강화 학습 문제를 풀기 위해서 수학적 모델을 하나 차용했는데, 그것이 바로 마코프 의사결정 과정(Markov decision process, MDP)입니다. <br>\n",
    "    MDP는 우리가 앞서 다룬 __순차적 의사결정(sequential decision) 문제를 다루기 위해 사용하는 일종의 수학적 기술__ 정도로 보면 될 것 같습니다. <br>\n",
    "    마르코프 결정 프로세스 (MDP) 는 순차적 의사결정을 하기 위한 아주 고전적인 방법론입니다. <br>\n",
    "    [출처 - [카카오AI리포트]알파고를 탄생시킨 강화학습의 비밀](https://brunch.co.kr/@kakao-it/73) <br>\n",
    "    [출처 - Markove Decision Process (1) - 개요](https://yjjo.tistory.com/23)\n",
    "<br><br>\n",
    "\n",
    "- __순차적 의사결정 문제 (Sequential Decision Making)__ \n",
    "<br><br>\n",
    "    In artificial intelligence, sequential decision making refers to __algorithms that take the dynamics of the world into consideration, thus delay parts of the problem until it must be solved__. <br>\n",
    "    It can be described as a procedural approach to decision-making, or as a step by step decision theory. Sequential decision making has as a consequence the intertemporal choice problem, __where earlier decisions influences the later available choices__. <br>\n",
    "    [출처 - 위키피디아](https://en.wikipedia.org/wiki/Sequential_decision_making) \n",
    "<br><br><br><br><br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 마르코프 프로세스 (Markov Process) \n",
    "<br>\n",
    "\n",
    "### 의미 <br>\n",
    "\n",
    "마르코프 프로세스는 <br>\n",
    "__미리 정의된__ 어떤 __확률 분포를 따라서__ 상태 사이를 이동해 가는 과정 \n",
    "<br><br>\n",
    "조금만 더 풀어서 써보면, <br>\n",
    "한 상태에서 다음 상태로 넘어가는데 각각 정해진 확률에 따라 다음 상태중 하나로 넘어감 <br>\n",
    "( 확률 분포이기 때문에, 각각 정해진 확률의 합 = 1 ) <br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "### 정의 <br>\n",
    "\n",
    "$MP \\equiv (S,P)$\n",
    "<br><br>\n",
    "\n",
    "- $S$ : 상태의 집합 <br>\n",
    "    가능한 상태들을 모두 모아놓은 집합 \n",
    "<br><br>\n",
    "- $P$ : 전이 확률 행렬<br>\n",
    "    $P_{ss'}$ 는 상대 $s$ 에서 다음 상태 $s'$ 에 도착할 확률 &ensp;&ensp; ... $P_{ss'} \\; = \\; \\mathbb{P} [S_{t+1} = s' | S_{t} = s]$ \n",
    "    <br><br>\n",
    "    - 전이 확률 '__행렬__' 인 이유 <br>\n",
    "    마르코프 프로세스에서 상태의 개수가 n 개인 경우, <br>\n",
    "    $P_{ss'}$ 의 값은 n x n 행렬의 형태로 표현이 됩니다 ! <br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "### 마르코프 성질 (Markov property) <br>\n",
    "\n",
    "마르코프 프로세스의 모든 상태는 마르코프 성질을 따릅니다. <br><br>\n",
    "\n",
    "- 마르코프 성질을 따른다는 의미 <br>\n",
    "    \"미래는 오로지 현재에 의해 결정된다.\" <br><br>\n",
    "\n",
    "- 마르코프 성질을 따른다는 정의 <br>\n",
    "    $\\mathbb{P}[s_{t+1}|s_{t}] \\; = \\; \\mathbb{P}[s_{t+1}|s_1,s_2,...,s_t]$ <br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "### 마르코프한 상태 vs 마르코프하지 않은 상태 <br>\n",
    "\n",
    "\\[ 함께 더 논의해 봅시다. \\]\n",
    "\n",
    "\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "#### 더 알아보기 <br>\n",
    "\n",
    "- __마르코프 프로세스 (Markov Process)__ \n",
    "<br><br>\n",
    "    __불확실한 상황 하에서 의사결정__을 하려면 '확률'에 기초해서 분석을 해야 합니다.  어떤 사건이 발생할 확률값이 시간에 따라 변화해 가는 과정을 확률적 과정(Stochastic Process)라고 하며, __확률적 과정 중에서 한 가지 특별한 경우가 마르코프 과정 (Markov Process)__ 입니다.\n",
    "    <br><br>\n",
    "     \"어떤 상태가 일정한 시간 간격으로 변하고, 다음 상태는 현재 상태에만 의존하여 확률적으로 변하는 경우, 이 상태의 변화를 마르코프 과정(Markov Process)이라 부른다.  마르코프 과정에서는 현재 상태에 따라서만 다음 상태가 결정되며, 현재 상태에 이르기까지의 과정은 전혀 고려할 필요가 없다.\" <br>\n",
    "    [출처 - [선형대수] 마아코프 과정 (Markov Process), 대각화(diagonalization) 적용하여 계산하기](https://rfriend.tistory.com/184)\n",
    "<br><br><br><br><br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 마르코프 리워드 프로세스 (Markov Reward Process) \n",
    "<br>\n",
    "\n",
    "\n",
    "### 의미 <br>\n",
    "\n",
    "마르코프 프로세스는 <br>\n",
    "미리 정의된 어떤 확률 분포를 따라서 상태 사이를 이동해 가는 과정 \n",
    "<br><br>\n",
    "마르코프 리워드 프로세스는 <br>\n",
    "마르코프 프로세스에서 __보상의 개념이 추가__된 과정 <br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "### 정의 <br>\n",
    "\n",
    "$MRP \\equiv (S,P,R,\\gamma)$\n",
    "<br><br>\n",
    "\n",
    "- $S$ : 상태의 집합 <br>\n",
    "    가능한 상태들을 모두 모아놓은 집합 <br><br>\n",
    "    \n",
    "- $P$ : 전이 확률 행렬<br>\n",
    "    $P_{ss'}$ 는 상대 $s$ 에서 다음 상태 $s'$ 에 도착할 확률 &ensp;&ensp; ... $P_{ss'} \\; = \\; \\mathbb{P} [S_{t+1} = s' | S_{t} = s]$ <br><br>\n",
    "\n",
    "- $R$ : 보상 함수 <br>\n",
    "    어떤 상태 $s$ 에 도착했을 때 받게 되는 보상 &ensp;&ensp; ... $R \\; = \\; \\mathbb{E}[R_{t}|S_{t}=s]$\n",
    "    <br>\n",
    "    - 기댓값인 이유 <br><br>\n",
    "\n",
    "- $\\gamma$ : 감쇠 인자 <br>\n",
    "    0 ~ 1 사이 수. 나중에 비해 현재 가까이에 얻는 보상이 얼마나 더 중요한지를 나타내는 파라미터. <br>\n",
    "    ( 미래시점의 일정금액과 동일한 가치를 갖는 현재시점의 금액(가치)을 계산하기 위해 적용하는 [할인율](https://terms.naver.com/entry.nhn?docId=3472958&cid=40942&categoryId=31913)과 비슷한 개념 ? )\n",
    "<br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "### 리턴 ($G$) <br>\n",
    "\n",
    "MRP 에서는 상태가 바뀔 때 마다 보상을 받습니다. <br>\n",
    "상태($s_t$) 에서 종료 상태 ($s_r$) 까지 하나의 에피소드가 끝날 때 까지 미래에 받을 보상의 합을 의미합니다. <br>\n",
    "이 때 보상의 합은 단순 덧셈 합이 아닌 감쇠된 보상의 합입니다. <br><br>\n",
    "\n",
    "$G_t \\; = \\; R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots$ <br>\n",
    "( 미래에 발생하게 될 현금흐름을 현재가치로 할인하여 모두 더한 값인 [순현재가치](https://terms.naver.com/entry.nhn?docId=3472964&cid=40942&categoryId=31913)와 비슷한 개념 ? ) <br><br>\n",
    "\n",
    "\n",
    "- 리턴은 미래의 보상 합 <br>\n",
    "    리턴의 수식에 시점 t 이전 과거의 보상은 없습니다. <br>\n",
    "    리턴은 과거는 신경 쓰지 않고 미래만 보는 노빠꾸 입니다. <br><br>\n",
    "\n",
    "- 한 에피소드 <br>\n",
    "    $s_0, R_0, \\quad s_1, R_1, \\quad ... , \\quad s_t, R_t, \\quad ... , \\quad s_r, R_r$ (종료) <br><br>\n",
    "\n",
    "\n",
    "\n",
    "- $\\gamma$ 를 사용하는 이유 <br>\n",
    "    1. 현재에서 가까운 시점의 보상을 더 선호 (중요하게 생각) <br>\n",
    "    2. 미래의 먼 시점의 보상에 대한 불확실성(리스크) 반영 <br>\n",
    "    3. 위 내용을 수학적으로 가중표현하기 편리하며, 종료가 없는 에피소드의 경우 등비수열의 합이 수렴하는 장점 <br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "### 강화학습의 목적 <br>\n",
    "\"강화학습은 보상을 최대화 하도록 학습하는 것이 목적 !\" $\\cdots$ (X) <br>\n",
    "\"강화학습은 __리턴을 최대화__ 하도록 학습하는 것이 목적 !\" $\\cdots$ (O)\n",
    "<br><br><br><br><br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<br>\n",
    "\n",
    "### 상태 가치 (State Value)\n",
    "\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "\n",
    "### 상태 가치 함수 (State Value Function)\n",
    "\n",
    "\n",
    "<br><br><br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}